# Meena - Pytorch 
Implementation of Meena for open domain conversation using pytorch. 
The model in this repo use vanilla Transformer seq2seq model (not Evolved Transformer).
The model consists of 1 encoder and 9 decoder.  

## Model
### Meena
Transformer seq2seq model.
![](./images/meena_architecture.png)

### Model Summary
- Total Prameters: **1.1B**(1,100,293,120)
```
-----------------------------------------------------------------------
      Layer (type)        Output Shape         Param #     Tr. Param #
=======================================================================
    MeenaEncoder-1      [1, 128, 2560]     104,604,160     104,604,160
    MeenaDecoder-2      [1, 128, 2560]     970,083,840     970,083,840
       LayerNorm-3      [1, 128, 2560]           5,120           5,120
          Linear-4     [1, 128, 10000]      25,600,000      25,600,000
=======================================================================
Total params: 1,100,293,120
Trainable params: 1,100,293,120
Non-trainable params: 0
-----------------------------------------------------------------------
```


## Vocab
This repo use **10K Wordpiece BPE**. Original Meena use sentencepiece library. Meena team use a vocabulary of **8K BPE**, which they found in early experiments to be sufficient for generating specific response. 


## Data
**Pretraining Data**  
- 34GB Korean corpus data 

**Test Pretrainig Data**  
- Kowiki(634MB) + Messanger data(63MB)

**Conversation Data**

[comment]: <> (- [[AI Hub] Free conversation voice &#40;normal men and women&#41;]&#40;https://aihub.or.kr/aidata/30703&#41;)

[comment]: <> (- [[AI Hub] Free conversation voice &#40;Children&#41;]&#40;https://aihub.or.kr/aidata/30705&#41;)

[comment]: <> (- [[AI Hub] Free conversation voice &#40;elderly men and women&#41;]&#40;https://aihub.or.kr/aidata/30704&#41;)

[comment]: <> (- [[AI Hub] Korean conversation]&#40;https://aihub.or.kr/aidata/85&#41;)

[comment]: <> (- [[AI Hub] In-vehicle conversation and command voice]&#40;https://aihub.or.kr/aidata/34177&#41;)
- [[AI Hub] Emotional conversation corpus](https://aihub.or.kr/aidata/7978)
- [[AI Hub] KETI, Korean conversation dataset](https://aihub.or.kr/opendata/keti-data/recognition-laguage/KETI-02-011)
- [[AI Hub] KETI, A one-shot conversation dataset with Korean emotion information](https://aihub.or.kr/opendata/keti-data/recognition-laguage/KETI-02-009)
## Pretraining
Pretrained on 34GB Korean corpus data. 
### Train Loss
- epoch: 1
- step: 2250000

![](./images/meena_pretrain_losses.png)

### Evaluation
- Total eval loss: 2.2944
- Total eval perplexity: 10.4958

```sh
2021-09-02 16:49:49.942686 | Step: 1557220 | Eval Loss: 2.294469305341254 | Perplexity: 10.495867182863075
```

## Fine-tuning
Fine-tuned on 94.8MB Korean Conversation Data

**Evaluation**

|  epoch  |   loss   |  Perplexity  |
|:-------:|----------|--------------|
|    1    |  2.2878  |    11.0814   |
|    2    |  2.2652  |    10.8460   |
|    3    |  2.2489  |    10.6738   |
|    4    |  2.2373  |    10.5701   |
|    5    |  2.2280  |    10.4907   |



## Checkpoint
### 1. Pretrained Meena
- it's preparing
### 2. Fine-tuned Meena
- it's preparing

## Device
- V100, 16G Memory
- Cuda 10.1, Driver 418.67
```
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 418.67       Driver Version: 418.67       CUDA Version: 10.1     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  Tesla V100-PCIE...  On   | 00000000:00:06.0 Off |                    0 |
| N/A   43C    P0   151W / 250W |  15590MiB / 16130MiB |     63%      Default |
+-------------------------------+----------------------+----------------------+
```

## Chat Example
### Test Pretrained Model
Model prtrained using kowiki & Messager data. 
- 3 epoch
- ./example/chat.py
#### Meta Data
  ```py
    meta_data ='[CLS] ì—¬ê°€ì™€ ì˜¤ë½ (ìœ í¥, ì·¨ë¯¸, ê´€ì‹¬ì‚¬, íœ´ì¼ í™œë™, ë™ì•„ë¦¬, ë™í˜¸íšŒ) [SEP] '
    meta_data +='A=20ëŒ€ ì—¬ì„± [SEP] '
    meta_data +='B=20ëŒ€ ì—¬ì„± [SEP] '
  ```
#### Example
- A: User
- B: KoMeena
```txt
ğŸ˜ ê³ ë¯¸ë‚˜ì—ê²Œ ë§ì„ ê±´ë„¤ì„¸ìš”!
A : ë‚ ë„ ë”ìš´ë° ì˜í™”ë‚˜ ë³´ëŸ¬ê°€ìã…‹ã…‹
B : ì–´ë””ì—ê°ˆê¹Œ
A : cgvë¡œ ê°ˆê¹Œ??
B : ì•„ë‹˜ ì§€ë‚˜?
A : ì§€ë‚˜ëŠ” ë­”ë°
B : 1986ë…„ì— ë°œë§¤ëœ ë¡¤í”Œë ˆì‰ ë¹„ë””ì˜¤ ê²Œì„ì´ë‹¤.
A : ìœ¼ìŒ ì˜í™”ëŠ” ì—†ë‹ˆ
B : ì˜í™”ëŠ” ëŒ€í•œë¯¼êµ­ì˜ ì˜í™” ì˜í™”, ë“œë¼ë§ˆ, ì• ë‹ˆë©”ì´í„° ë“±ì€ ë°°ìš° ì–‘ì„±ìœ¼ë¡œ ë“£ê³  ìˆì§€ë§Œ, ì£¼ë§ì— ê·¸ë…€ëŠ” ëŒ€ë¶€ë¶„ì´ ì˜í™”ë¥¼ ì¢‹ì•„í•œë‹¤.
A : ìœ„í‚¤ìŠ¤ëŸ¬ìš´ ë§ì„ í•˜ëŠ”êµ¬ë‚˜
B : 2011ë…„ í•˜ë°˜ê¸°ì˜ í´ëŸ½ í€¸ ì—˜ë¦¬ìë² ìŠ¤ 2ì„¸ ì‹œê¸°ì´ë‹¤.
A : ë§¥ë½ë„ ì—†êµ¬
B : ë§¥ë½ì´ ë¬´ëª…ì´ì—¬ì¹˜ì§€
A : ë°°ê³ í”„ë‹¤ ì´ëˆ”ì•„
B :
A : ì™œ ë§ ì—†ëƒ
B : ìœ¼ã…“ìœ¼ìœ¼ìœ¼ìœ¼ìœ¼ìœ¼ìœ¼!!!!!!! ~!
A : ë¹„ëª… ì§€ë¥´ì§€ ë§ˆã…‹ã…‹ã…‹ã…‹ã…‹
B : ã…‡0ã…‡ ì´ì „ ì œëª©ì´ë‹¤
```


## TODO
- [ ] ë°ì´í„° ì¶”ê°€ ì „ì²˜ë¦¬

## Test
Three type model tested 
- â‘  Only Decoder Model: only transformer decoder(ex. GPT)
- â‘¡ Seq2Seq Model: Seq2seq model like original transformer model
- â‘¢ Stack Model: Stack 1 Encoder + 12 Decoder
