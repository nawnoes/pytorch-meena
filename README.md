# KoMeena
Korean Meena for open domain conversation

## Model
My transformer model.

## Vocab
Meena's vocab sentencepiece library. Meena team use a vocabulary of **8K BPE**, which they found in early experiments to be sufficient for generating specific response. 
This project use **10K or 22K BPE**. 

## Data
- Pretraining Data: 34GB Korean corpus data 
- Test Pretrainig Data: Kowiki(634MB) + Messanger data(63MB)

## Chat Example
### Test Pretrained Model
Model prtrained using kowiki & Messager data. 
- 3 epoch
- ./example/chat.py
#### Meta Data
  ```py
    meta_data ='[CLS] ì—¬ê°€ì™€ ì˜¤ë½ (ìœ í¥, ì·¨ë¯¸, ê´€ì‹¬ì‚¬, íœ´ì¼ í™œë™, ë™ì•„ë¦¬, ë™í˜¸íšŒ) [SEP] '
    meta_data +='A=20ëŒ€ ì—¬ì„± [SEP] '
    meta_data +='B=20ëŒ€ ì—¬ì„± [SEP] '
  ```
#### Example
- A: User
- B: KoMeena
```txt
ğŸ˜ ê³ ë¯¸ë‚˜ì—ê²Œ ë§ì„ ê±´ë„¤ì„¸ìš”!
A : ë‚ ë„ ë”ìš´ë° ì˜í™”ë‚˜ ë³´ëŸ¬ê°€ìã…‹ã…‹
B : ì–´ë””ì—ê°ˆê¹Œ
A : cgvë¡œ ê°ˆê¹Œ??
B : ì•„ë‹˜ ì§€ë‚˜?
A : ì§€ë‚˜ëŠ” ë­”ë°
B : 1986ë…„ì— ë°œë§¤ëœ ë¡¤í”Œë ˆì‰ ë¹„ë””ì˜¤ ê²Œì„ì´ë‹¤.
A : ìœ¼ìŒ ì˜í™”ëŠ” ì—†ë‹ˆ
B : ì˜í™”ëŠ” ëŒ€í•œë¯¼êµ­ì˜ ì˜í™” ì˜í™”, ë“œë¼ë§ˆ, ì• ë‹ˆë©”ì´í„° ë“±ì€ ë°°ìš° ì–‘ì„±ìœ¼ë¡œ ë“£ê³  ìˆì§€ë§Œ, ì£¼ë§ì— ê·¸ë…€ëŠ” ëŒ€ë¶€ë¶„ì´ ì˜í™”ë¥¼ ì¢‹ì•„í•œë‹¤.
A : ìœ„í‚¤ìŠ¤ëŸ¬ìš´ ë§ì„ í•˜ëŠ”êµ¬ë‚˜
B : 2011ë…„ í•˜ë°˜ê¸°ì˜ í´ëŸ½ í€¸ ì—˜ë¦¬ìë² ìŠ¤ 2ì„¸ ì‹œê¸°ì´ë‹¤.
A : ë§¥ë½ë„ ì—†êµ¬
B : ë§¥ë½ì´ ë¬´ëª…ì´ì—¬ì¹˜ì§€
A : ë°°ê³ í”„ë‹¤ ì´ëˆ”ì•„
B :
A : ì™œ ë§ ì—†ëƒ
B : ìœ¼ã…“ìœ¼ìœ¼ìœ¼ìœ¼ìœ¼ìœ¼ìœ¼!!!!!!! ~!
A : ë¹„ëª… ì§€ë¥´ì§€ ë§ˆã…‹ã…‹ã…‹ã…‹ã…‹
B : ã…‡0ã…‡ ì´ì „ ì œëª©ì´ë‹¤
```

## Issue
- seq2seq ëª¨ë¸ì—ì„œ Lossê°€ 0ìœ¼ë¡œ ìˆ˜ë ´í•˜ëŠ” ë¬¸ì œ.
    1. ì¸ì½”ë”© ë¬¸ì œì¸ì§€?
    2. encdoer inputê³¼ decoder inputì´ ë™ì¼í•´ì„œ ë°œìƒí•˜ëŠ”ê²ƒìœ¼ë¡œ ì˜ˆìƒ
        - [2021.07.05] Encoderì˜ ì…ë ¥ê³¼ Decoderì˜ ì…ë ¥ì„ ë‹¤ë¥´ê²Œ ë³€ê²½í•´ì„œ í…ŒìŠ¤íŠ¸
        - [2021.07.12] í•™ìŠµ ì •ìƒ ë™ì‘ í™•ì¸.
- ì„±ëŠ¥ ë¬¸ì œ
  **ê°œì„ ë°©ë²•**  
    
    - â‘  Pretraining í›„ ì ì€ ëŒ€í™” ë°ì´í„°ì— ëŒ€í•´ Fine-tuning
    - â‘¡ ëŒ€í™” ë°ì´í„° ìˆ˜ì§‘
    
- ëŒ€í™” ë°ì´í„° ìˆ˜ì§‘ í•„ìš”
    
## Test
Three type model tested 
- â‘  Only Decoder Model: only transformer decoder(ex. GPT)
- â‘¡ Seq2Seq Model: Seq2seq model like original transformer model
- â‘¢ Stack Model: Stack 1 Encoder + 12 Decoder